#!/bin/bash
"exec" "`dirname $0`/python3" "$0" "$@"

"""Syntax: gridlabd geodata OPTIONS DIRECTIVE [ARGUMENTS]

The geodata command gathers and joins geographic data. The geodata subcommand
uses directives that are documented in the DIRECTIVES section below.

In general geodata is used to acquire geographic information at a location or
along a specified path. This information includes ground elevation, vegetation
characteristics, weather, census, building, and transportation data.  The
specific data sets and their origins are described in the DATASETS section
below.

OPTIONS

  [-C|--config [+]FILE]       load config file (use '+' appends to current
                              configuration)
  [--clean]                   clean the cache files
  [-d|--debug]                enable debug output, including error traceback
  [-f|--format FORMAT]        change output format
  [--fieldsep STRING]         set the RAW output field separator
  [--filter=SPECS]            apply dataframe functions as filter
  [-j|--join TYPE]            control how dataset joins with input path
  [-k|--key KEY]              change method for generating keys
  [-o|--output CSVOUT]        output to CSVOUT
  [-p|--precision INT]        change precision of location values
  [-r|--resolution METERS]    change the resolution at which positions are
                              merged
  [--recordsep STRING]        change the RAW output record separator
  [-s|--silent]               disable error output
  [--select=COLUMNS]          select only rows that have true values for column(s)
  [--show_config]             display the current configuration
  [--show_options]            display the current options
  [-T|--threadcount THREADS]  change maximum thread count
  [--units UNIT]              change units of distance
  [-v|--verbose]              enable verbose output
  [-w|--warning]              disable warning output

REMARKS

The default cache folder is /usr/local/opt/gridlabd/share/gridlabd/geodata. The default
config file $HOME/.gridlabd/geodata-config. The default maximum thread count
is 1.

DIRECTIVES

The following directives are available:

  config get PARAMETER
  config set [SCOPE:]PARAMETER VALUE
  config unset [SCOPE:]PARAMETER
  config show

    The config directive changes values in the config file. Valid scopes are
    "system", "user", or "local", which determines where the configuration
    values are stored, e.g., "$GLD_ETC/geodata/geodata.conf",
    "$HOME/.gridlabd/geodata.conf", or "./geodata.conf", respectively.

  create [ARGUMENTS]

    The create directive generates a new data from the specified
    dataset.

  merge [-D|--dataset] NAME [ARGUMENTS]

    The merge diretive merges the results from the specified dataset with the
    data.  If a column exists, it is updated. If a column does not exist, it is
    created.

    The -D|--dataset directive specifies the dataset from which the geographic
    information is to be acquired. See the DATASETS section below for details.

ARGUMENTS

All directives support the following arguments

  <name>=<value>[+<name>=<value>[+...]]

    Specifies one or more name/value tuples from which to construct a dataframe.
    Note that by default all fields are string.  Use CSV format inputs to use
    automatic types.

  CSVIN [...]

    Specifies one or more files from which CSV input is read. If no values or
    CSV input files are specified, the CSV data input is read from /dev/stdin.

  [-o|--output] CSVOUT

    Normally the output is written to /dev/stdout. When CSVOUT is specified the
    output data is written to the specified file. If the output file already
    exists, it is overwritten with the new data.

KEYS

The [-k|--key KEY] options changes how row keys are generated. The default is
the record id in the order in which it was read. This key is only defined for
records actually read is.  If resolution is used the "id" key is not defined for
records added, and if the key "id" is used at any point, the added records will
be lost.

  [-k|--key "location"]

    The "location" key computes geohash codes. Rows are indexed on the key named
    "location".

  [-k|--key "position"]

    The "position" key performs a distance calculation with respect to the first
    entry in the data, thus maintaining the ordering the rows.  Rows are indexed
    on the key names "position".

  [-k|--key "uuid"]

    The "uuid" key generates a universally unique identifier for each record.
    Rows are indexed on the key name "uuid".

If the [-r|--resolution METERS] option is used, the keys are generated at
the specified resolution along the path. The default resolution is None. If
resolution is specified without a key, then "position" is used.

DATASETS

The following datasets are available. To specify a layer, use the syntax
"NAME.LAYER".  Multiple datasets and layers may be specified using a
comma-delimited list.

  building

    The building dataset provides build environment data from NREL. Available
    layers are "building_type" and "building_size".

  census

    The census dataset provides economic and population data from the US Census
    Bureau.

  distance

    The distance dataset calculates the distance between each point and the
    point before it.

  elevation

    The elevation dataset provides 1 arcsecond resolution ground elevation data
    from the USGS.

  powerline

    The powerline dataset provides powerline geometry calculations based on
    the GridLAB-D cable library.

  transportation

    The transportation dataset provides mobility data from traffic data services.
    You must subscribe to the TrafficView service to use this dataset.

  utility

    The utility data provides data about the utility servicing the locations.

  vegetation

    The vegetation dataset provides 7 layers of data about vegetation.  Available
    layers are "canopy_cover", "canopy_height", "base_height", "bulk_density",
    "layer_count", "fuel_density", and "surface_fuels".. You must subscribe to the
    Forest Observatory service to use this dataset.

  weather

    The weather dataset provides historical, current, and short term forecasts of
    dry bulk, wet bulb, solar, and wind data from NOAA.

FORMATS

Output is generated by default in CSV format.  Other output formats may be
selected as follows:

  CSV[:<fields>]    fields are output in CSV format

  JSON[:<fields>]   fields are output in JSON format

  RAW[:<fields>]    fields are output as a raw string

  GLM[:[@]<fields>] fields are output as GLM objects. If @ included, output is
                    is GLM modify statements. Note that invalid field names do
                    not raise a KeyError exception or error message. This is
                    because CSV tables may contain multiple classes with some
                    columns that are not defined for every class.

  TABLE[:[*][<fields>]]
                    fields are output as a human readable table.  Note that if
                    too many columns or rows will be output, then intermediate
                    values will be suppressed and replaced with ellipses unless
                    the field string starts with '*'.

  PLOT[:<fields>]   fields are plotted to a matplotlib figure.  Fields may be one
                    'location', 'position', or 'geometry', which plot points, lines,
                    and areas, respectively.

  XLSX[:<fields>]   outputs the data as an Excel spreadsheet.

  GDF[:<fields>]    outputs the data as a GeoPandas file.

DEVELOPER API

Packages must implement the following (TODO indicates where your code goes)

    version = 1

    default_options = {
        TODO,
    }

    default_config = {
        TODO,
    }

    def apply(data, options=default_options, config=default_config):
        result = TODO
        return pandas.DataFrame(result)

    if __name__ == '__main__':
        import unittest
        class TestAddress(unittest.TestCase):
            def test_1(self):
                TODO
        unittest.main()

See geodata_distance.py for a simple example of a dataset implementation.

EXAMPLES

1) Creating dataset and output formats

    To create a new dataset from the command line and output as CSV:

        $ gridlabd geodata create name=addr1+latitude=37.4150+longitude=-122.2056+class=example \
        > name=addr2+latitude=37.3880+longitude=-122.2884+class=example
        id,name,latitude,longitude,class
        0,addr1,37.4150,-122.2056,example
        1,addr2,37.3880,-122.2884,example

    To output a single field:

        $ gridlabd geodata create name=addr1+latitude=37.4150+longitude=-122.2056+class=example \
        > name=addr2+latitude=37.3880+longitude=-122.2884+class=example -f CSV:name
        id,name
        0,addr1
        1,addr2

    To output multiple fields:

        $ gridlabd geodata create name=addr1+latitude=37.4150+longitude=-122.2056+class=example \
        > name=addr2+latitude=37.3880+longitude=-122.2884+class=example -f CSV:name,class
        id,name,class
        0,addr1,example
        1,addr2,example

    Create the same data, but output JSON

        $ gridlabd geodata create name=addr1+latitude=37.4150+longitude=-122.2056+class=example \
        > name=addr2+latitude=37.3880+longitude=-122.2884+class=example -f JSON
        [{"name":"addr1","latitude":"37.4150","longitude":"-122.2056","class":"example"},{"name":"addr2","latitude":"37.3880","longitude":"-122.2884","class":"example"}]

    Same data, but output as GLM

        $ gridlabd geodata create name=addr1+latitude=37.4150+longitude=-122.2056+class=example \
        > name=addr2+latitude=37.3880+longitude=-122.2884+class=example -f GLM
        // generated by `/usr/local/opt/gridlabd/4.2.20-210422-develop_geodata_subcommand/bin/gridlabd-geodata create name=addr1+latitude=37.4150+longitude=-122.2056+class=example name=addr2+latitude=37.3880+longitude=-122.2884+class=example -f GLM` at 2021-04-22 08:38:37.163874
        object example
        {
        	name "{row['name']}";
        	latitude "37.4150";
        	longitude "-122.2056";
        }
        object example
        {
        	name "{row['name']}";
        	latitude "37.3880";
        	longitude "-122.2884";
        }

    Same data, but output GLM modify statements

        $ gridlabd geodata create name=addr1+latitude=37.4150+longitude=-122.2056+class=example \
        > name=addr2+latitude=37.3880+longitude=-122.2884+class=example -f GLM:@latitude,longitude
        // generated by `/usr/local/opt/gridlabd/4.2.20-210422-develop_geodata_subcommand/bin/gridlabd-geodata create name=addr1+latitude=37.4150+longitude=-122.2056+class=example name=addr2+latitude=37.3880+longitude=-122.2884+class=example -f GLM:@latitude,longitude` at 2021-04-22 08:38:49.759341
        modify addr1.longitude "-122.2056";
        modify addr2.longitude "-122.2884";

2) Merging a geographic dataset into an exist dataframe

The following command uses to address dataset on locations provided on the command line:

    $ gridlabd geodata merge -D address name=addr1+latitude=37.4150+longitude=-122.2056+class=example \
    > name=addr2+latitude=37.3880+longitude=-122.2884+class=example
    id,name,latitude,longitude,class,address
    0,addr1,37.4150,-122.2056,example,"Stanford Linear Accelerator Center National Accelerator Laboratory, Sand Hill Road, Menlo Park, San Mateo County, California, 94028, United States"
    1,addr2,37.3880,-122.2884,example,"Allen Road, San Mateo County, California, United States"

The following command merges addresses into an existing dataframe

    $gridlabd geodata create name=addr1+latitude=37.4150+longitude=-122.2056+class=example \
    > name=addr2+latitude=37.3880+longitude=-122.2884+class=example | gridlabd geodata merge -D address
    id,name,latitude,longitude,class,address
    0,addr1,37.415,-122.2056,example,"Stanford Linear Accelerator Center National Accelerator Laboratory, Sand Hill Road, Menlo Park, San Mateo County, California, 94028, United States"
    1,addr2,37.388,-122.2884,example,"Allen Road, San Mateo County, California, United States"

3) GLM usage

The following example uses the geodata subcommand to modify GridLAB-D objects with
data acquired from the address dataset.

    clock {
        starttime "2020-01-01 00:00:00";
        stoptime "2021-01-01 00:00:00";
    }
    module residential;
    class house
    {
        char1024 address;
    }

    object house
    {
        address "2575 Sand Hill Rd., Menlo Park, CA";
    }

    object house
    {
        address "2000 Broadway, Redwood City, CA";
    }

    // write house.address to csv
    #write /tmp/write.csv house:address

    // get lat,lon from address and write to csv
    #geodata merge -D address /tmp/write.csv --reverse --format GLM:@latitude,longitude -o /tmp/read.glm

    // read csv as modify
    #include "/tmp/read.glm"

    #set savefile="/tmp/final.json"

After this GridLAB-D model is loaded and run, the output json file can be read using the subcommand, e.g.,

    $ gridlabd json-get objects house:0 latitude </tmp/final.json
    37.415463

"""
import sys, os
import importlib
from importlib import util
import json
import math, numpy
import pandas
import haversine
import geopandas
import matplotlib.pyplot as plt
import uuid
import folium

NAME = "geodata"
VERSION = "0.0.0"
GEODATA = sys.modules[__name__]
DIRECTIVES = ["config","create","merge"]
DATASET = None
OUTPUT = "/dev/stdout"
INPUT = "/dev/stdin"
MODULE = None
PKGDATA = os.getenv("GLD_ETC")
if not PKGDATA:
    PKGDATA = "/usr/local/opt/gridlabd/share/gridlabd"
VALID_FORMATS = ["CSV","RAW","JSON","POS","GLM","FIELD","POS","TABLE","PLOT","GDF","XLSX"]

CONFIGFILE = "geodata.conf"
CONFIGDIR = {
    "system" : PKGDATA + "/geodata",
    "user" : os.getenv("HOME") + "/.gridlabd/geodata",
    "local" : os.getenv("PWD"),
}
CONFIG = {
    "geodata_url" : "http://geodata.gridlabd.us/",
    "output_format" : "CSV",
    "path_join" : "outer",
    "column_names" : {
        'ID' : "id",
        'UUID' : "uuid",
        'LAT' : "latitude",
        'LON' : "longitude",
        'DIST' : "distance",
        'HEAD' : "heading",
        'LOC' : "location",
        'POS' : "position",
    },
    "uuid_type" : 4, # valid UUID key type values are 1 (host, seq, time) and 4 (random)
}

OPTIONS = {
    "verbose" : False,
    "debug" : False,
    "silent" : False,
    "quiet" : False,
    "warning" : True,
    "key_index" : "",
    "max_threads" : 1,
    "config_file" : CONFIGFILE,
    "field_separator" : " ",
    "record_separator" : "\n",
    "input_delimiter" : "+",
    "warning_as_error" : False,
    "plot" : {
        "figsize" : [7.0,5.0],
        "cmap" : "",
        "categorical" : False,
        "legend" : False,
        "scheme" : "",
        "k" : 5,
        "vmin" : float('nan'),
        "vmax" : float('nan'),
        "aspect" : 'auto',
    },
    "show" : {
        "block" : True,
    },
    "savefig" : {
        "dpi" : 300,
        "facecolor" : "w",
        "edgecolor" : "k",
        "orientation" : "landscape",
        "format" : "png",
        "transparent" : False,
        "pad_inches" : 0.1,
    },
    "table" : {
        "max_rows" : 10,
        "max_columns" : 10,
        "width" : 80,
        "max_colwidth" : 16,
    },
    "precision" : {
        "distance" : 0,
        "heading" : 1,
        "geolocation" : 5,
        "id" : 0,
        "resolution_id" : 3,
    },
    "resolution" : 0,
    "resolution_id" : False,
    "json" : {
        "orient" : "index",
        "date_format" : "iso",
        "double_precision" : 10,
        "force_ascii" : True,
        "date_unit" : "s",
        "indent" : 0,
    },
    "filter" : "",
    "select" : "",
}

E_OK = 0
E_SYNTAX = 1
E_NOTFOUND = 2
E_MISSING = 3
E_INVALID = 4
E_FAILED = 5
E_WARNING = 8
E_NOTIMPLEMENTED = 9

def format_message(type,msg):
    if DATASET:
        return f"{type} [{NAME}/{DATASET}]: {msg}"
    else:
        return f"{type} [{NAME}]: {msg}"

def error(msg,exitcode=None):
    if OPTIONS['debug']:
        raise Exception(msg)
    if not OPTIONS['silent']:
        print(format_message("ERROR",msg),file=sys.stderr,flush=True)
    if exitcode:
        exit(exitcode)

last_warning = None
warning_count = 0
def warning(msg):
    global last_warning
    global warning_count
    if OPTIONS["warning_as_error"]:
        error(msg,E_WARNING)
    if last_warning != msg and OPTIONS['warning']:
        print(format_message("WARNING",msg),file=sys.stderr,flush=True)
        last_warning = msg
        warning_count = 1
    elif last_warning == msg:
        warning_count += 1

def verbose(msg):
    if OPTIONS['verbose']:
        print(format_message("VERBOSE",msg),file=sys.stderr,flush=True)

def output(msg):
    if not OPTIONS['quiet']:
        print(msg,file=sys.stdout)

def debug(msg):
    if OPTIONS['debug']:
        print(format_message("DEBUG",msg),file=sys.stderr,flush=True)

python_help = help
def help(dataset=None):
    if dataset:
        MODULE = load_dataset(dataset)
        if MODULE:
            python_help(MODULE.__name__)
        else:
            error(f"command '{command}' not found",E_SYNTAX)
    else:
        python_help(__name__)
    return None, None

def syntax():
    print(__doc__.split("\n")[0],file=sys.stdout)

def dataframe_to_table(data):
    table = f"{data}"
    rule = "-"*max(map(lambda x:len(x),table.split("\n")))
    return f"\n{rule}\n{table}\n{rule}\n"

#
# GEOHASH support
#
from math import log10

#  Note: the alphabet in geohash differs from the common base32
#  alphabet described in IETF's RFC 4648
#  (http://tools.ietf.org/html/rfc4648)
__base32 = '0123456789bcdefghjkmnpqrstuvwxyz'
__decodemap = { }
for i in range(len(__base32)):
    __decodemap[__base32[i]] = i
del i

def decode_exactly(geohash):
    """
    Decode the geohash to its exact values, including the error
    margins of the result.  Returns four float values: latitude,
    longitude, the plus/minus error for latitude (as a positive
    number) and the plus/minus error for longitude (as a positive
    number).
    """
    lat_interval, lon_interval = (-90.0, 90.0), (-180.0, 180.0)
    lat_err, lon_err = 90.0, 180.0
    is_even = True
    for c in geohash:
        cd = __decodemap[c]
        for mask in [16, 8, 4, 2, 1]:
            if is_even: # adds longitude info
                lon_err /= 2
                if cd & mask:
                    lon_interval = ((lon_interval[0]+lon_interval[1])/2, lon_interval[1])
                else:
                    lon_interval = (lon_interval[0], (lon_interval[0]+lon_interval[1])/2)
            else:      # adds latitude info
                lat_err /= 2
                if cd & mask:
                    lat_interval = ((lat_interval[0]+lat_interval[1])/2, lat_interval[1])
                else:
                    lat_interval = (lat_interval[0], (lat_interval[0]+lat_interval[1])/2)
            is_even = not is_even
    lat = (lat_interval[0] + lat_interval[1]) / 2
    lon = (lon_interval[0] + lon_interval[1]) / 2
    return lat, lon, lat_err, lon_err

def decode(geohash):
    """
    Decode geohash, returning two strings with latitude and longitude
    containing only relevant digits and with trailing zeroes removed.
    """
    lat, lon, lat_err, lon_err = decode_exactly(geohash)
    # Format to the number of decimals that are known
    lats = "%.*f" % (max(1, int(round(-log10(lat_err)))) - 1, lat)
    lons = "%.*f" % (max(1, int(round(-log10(lon_err)))) - 1, lon)
    if '.' in lats: lats = lats.rstrip('0')
    if '.' in lons: lons = lons.rstrip('0')
    return lats, lons

def encode(latitude, longitude, precision=12):
    """
    Encode a position given in float arguments latitude, longitude to
    a geohash which will have the character count precision.
    """
    lat_interval, lon_interval = (-90.0, 90.0), (-180.0, 180.0)
    geohash = []
    bits = [ 16, 8, 4, 2, 1 ]
    bit = 0
    ch = 0
    even = True
    while len(geohash) < precision:
        if even:
            mid = (lon_interval[0] + lon_interval[1]) / 2
            if longitude > mid:
                ch |= bits[bit]
                lon_interval = (mid, lon_interval[1])
            else:
                lon_interval = (lon_interval[0], mid)
        else:
            mid = (lat_interval[0] + lat_interval[1]) / 2
            if latitude > mid:
                ch |= bits[bit]
                lat_interval = (mid, lat_interval[1])
            else:
                lat_interval = (lat_interval[0], mid)
        even = not even
        if bit < 4:
            bit += 1
        else:
            geohash += __base32[ch]
            bit = 0
            ch = 0
    return ''.join(geohash)

def get_latlon(pos):
    """Convert a lat,lon string to a float pair"""
    try:
        latlon = pos.split(",")
        lat = float(latlon[0])
        lon = float(latlon[1])
        if -90 <= lat <= 90 and -180 <= lon <= 180:
            return lat,lon
        else:
            return None, None
    except:
        return None, None

def set_location(data):
    """Update the location index on the data"""
    try:
        result = pandas.DataFrame(data)
        result[CONFIG['column_names']['LOC']] = list(map(lambda pos: encode(float(pos[0]),float(pos[1])),zip(result[CONFIG['column_names']['LAT']],result[CONFIG['column_names']['LON']])))
        result.set_index(CONFIG['column_names']['LOC'],inplace=True)
    except:
        result = data
    return result

def set_position(data):
    """Update the path position index from the distance column"""
    if CONFIG['column_names']['DIST'] not in data.columns:
        result = pandas.DataFrame(data).merge(set_distance(data),how="outer")
    else:
        result = pandas.DataFrame(data)
    result[CONFIG['column_names']['POS']] = pandas.Series(result[CONFIG['column_names']['DIST']]*10**OPTIONS['precision']["id"],dtype="int32")
    result = result.set_index(CONFIG['column_names']['POS']).sort_index()
    return result

def set_uuid(data):
    if CONFIG['column_names']['UUID'] not in data.columns:
        try:
            data[CONFIG['column_names']['UUID']] = list(map(lambda x: getattr(uuid,f"uuid{CONFIG['uuid_type']}")().hex, range(len(data))))
        except Exception as err:
            error(f"uuid key generation error ({err})",E_INVALID)
    return data.set_index(CONFIG['column_names']['UUID'])

def set_index(data,index=None,silent=False):
    """Set the specified index on the data"""

    global OPTIONS
    if not index:
        index = OPTIONS['key_index']

    global CONFIG
    data[CONFIG['column_names']['LAT']] = data[CONFIG['column_names']['LAT']].map(lambda x: round(float(x),OPTIONS['precision']["geolocation"]))
    data[CONFIG['column_names']['LON']] = data[CONFIG['column_names']['LON']].map(lambda x: round(float(x),OPTIONS['precision']["geolocation"]))

    if OPTIONS['resolution'] and index != CONFIG['column_names']['POS']:
        warning(f"using index '{index}' cannot be used with '--resolution={OPTIONS['resolution']}' option")
    if OPTIONS['resolution_id'] and CONFIG['column_names']['ID'] in data.columns and CONFIG['column_names']['DIST'] in data.columns:
        d = data[CONFIG['column_names']['DIST']].max()
        data[CONFIG['column_names']['ID']] = (data[CONFIG['column_names']['DIST']]/d).round(OPTIONS['precision']['resolution_id'])
    if not index:
        if data.index.name:
            data.reset_index(inplace=True)
        if CONFIG['column_names']['ID'] in data.columns:
            data.set_index(CONFIG['column_names']['ID'],inplace=True)
        else:
            data.index.name = CONFIG['column_names']['ID']
        return data
    elif data.index.name == index:
        return data
    elif index == CONFIG['column_names']['LOC']:
        return set_location(data)
    elif index == CONFIG['column_names']['POS']:
        return set_position(data)
    elif index == CONFIG['column_names']['UUID']:
        return set_uuid(data)
    elif not type(index) is type(None):
        columns = index.split(",")
        try:
            return data.set_index(columns)
        except:
            if not silent:
                warning(f"unable to set index to '{index}'")
            return data
    elif index in data.columns:
        return data.set_index(index)
    else:
        error("index '{index}' is not valid",E_INVALID)

def get_subvalue(name,data):
    if type(name) is list:
        if len(name) > 1:
            result = get_subvalue(name[1:],data[name[0]])
        else:
            result = data[name[0]]
    else:
        result = data[name]
    return result

def set_subvalue(name,data,value):
    if type(name) is list:
        if len(name) > 1:
            result = set_subvalue(name[1],data[name[0]],value)
        else:
            result = set_subvalue(name[0],data,value)
    else:
        result = data[name]
        if type(result) is dict:
            data.update({name:value})
        else:
            data[name] = value
    return result

def set_value(arg,data,tags=[]):
    try:
        specs = arg.split('=')
        tags = specs[0].split('.')
        if len(specs) == 1: # boolean flags
            value = not get_subvalue(tags,data)
        else: # typed value
            value = '='.join(specs[1:])
            default = get_subvalue(tags,data)
            value_type = type(default)
            if value_type is type(None):
                raise Exception(f"the type/class of '{default.__repr__()}' is not permitted for config or option {'.'.join(tags).__repr__()}, try using an empty string for the default")
            elif value_type is list:
                items = value.split(',')
                value = []
                if len(items) != len(default):
                    raise Exception(f"the length of {value} is not the same as the length of {arg}")
                for n in range(len(items)):
                    value_type = type(default[n])
                    value.append(value_type(items[n]))
            else:
                value = value_type(value)
        set_subvalue(tags,data,value)
        result = True
    except:
        result = False
    return result

def update_values(arg,data):
    """Updates config or option values from data"""
    for key,value in data.items():
        if key in arg.keys():
            if type(arg[key]) is dict:
                arg[key].update(value)
                continue
        arg[key] = value
    return arg

def show_values(data,root=[]):
    for key, value in data.items():
        from copy import copy
        base = copy(root)
        base.append(key)
        if type(value) is dict:
            show_values(value,base)
        elif type(value) is list:
            print(f"--{'.'.join(base)}={'.'.join(list(map(lambda x: str(x),value)))}")
        else:
            print(f"--{'.'.join(base)}={value.__repr__()}")

def get_args(args):
    """Get the arguments, options, and configuration"""

    data = []

    global OPTIONS
    global CONFIG
    global MODULE
    if MODULE:
        update_values(OPTIONS,MODULE.default_options)
        update_values(CONFIG,MODULE.default_config)
    for arg in args:
        lat,lon = get_latlon(arg)
        opts = [None]
        if arg[0:2] == "--" and len(arg) > 2:
            opts = arg[2:].split("=")
            verbose(f"arg={arg} --> option {opts} accepted")
            if not set_value(arg[2:],OPTIONS) and not set_value(arg[2:],CONFIG):
                error(f"config/option '{arg}' is not valid",E_INVALID)
        elif type(lat) == float and type(lon) == float:
            data.append(pandas.DataFrame({CONFIG['column_names']['LAT']:[lat],CONFIG['column_names']['LON']:[lon]}))
            verbose(f"arg={arg} --> append {len(data[-1])} rows from args")
        elif os.path.exists(arg):
            if arg.endswith(".csv"):
                data.append(pandas.read_csv(arg))
            elif arg.endswith(".json"):
                data.append(pandas.read_json(arg,orient=OPTIONS['json']["orient"]))
            elif arg.endswith(".xlsx"):
                data.append(pandas.read_excel(arg))
            else:
                error(f"file '{arg}' is not a known format (unknown extension)",E_INVALID)
            verbose(f"arg={arg} --> append {len(data[-1])} rows from file")
        else:
            items = arg.split(OPTIONS['input_delimiter'])
            values = {}
            for item in items:
                specs = item.split("=")
                if len(specs) == 1:
                    global DATASET
                    if DATASET is None:
                        error(f"'{arg}' is not recognized",E_SYNTAX)
                    else:
                        values[DATASET] = [specs[0]]
                else:
                    values[specs[0]] = [specs[1]]
            data.append(pandas.DataFrame(values))
            verbose(f"arg={arg} --> append {len(data[-1])} rows from string")
    if not data:
        data = pandas.read_csv("/dev/stdin")
        verbose(f"no data; reading data from /dev/stdin")
    else:
        data = pandas.concat(data,ignore_index=True)
        verbose(f"{len(data)} rows total received")
    if not CONFIG['column_names']['ID'] in data.columns:
        data[CONFIG['column_names']['ID']] = range(len(data))
        data = data.astype({CONFIG['column_names']['ID']:"int32"})
        verbose(f"creating row index")

    if not OPTIONS['key_index'] and "resolution" in OPTIONS.keys() and OPTIONS['resolution']:
        OPTIONS['key_index'] = CONFIG['column_names']['POS']
    if OPTIONS['key_index'] in [CONFIG['column_names']['POS'],CONFIG['column_names']['LOC'],CONFIG['column_names']['ID']]:
        data = set_index(data)
        verbose(f"setting index to {OPTIONS['key_index']}")
    verbose(f"data = {dataframe_to_table(data)}")
    verbose(f"options = {json.dumps(OPTIONS,indent=4)}")
    verbose(f"config = {json.dumps(CONFIG,indent=4)}")
    return data,OPTIONS,CONFIG

def merge(args):
    """Merge directive implementation"""
    data,options,config = get_args(args)
    try:
        result = MODULE.apply(data,options,config,warning)
    except Exception as err:
        result = err
    if issubclass(type(result),Exception) :
        if OPTIONS['debug']:
            import traceback
            print("Traceback (most recent call last):")
            traceback.print_tb(result.__traceback__)
            print(f"Exception: {str(result)}")
        else:
            error(str(result),E_FAILED)
        result = None
    elif type(result) in [pandas.DataFrame,geopandas.GeoDataFrame]:
        result = set_index(result)
    else:
        warning(f"merge result is not a recognized object type (type is '{type(result)}')")
        result = None
    debug(f"merge(args={args.__repr__()}) --> data = {dataframe_to_table(result)}options = {json.dumps(options,indent=4)}\nconfig = {json.dumps(config,indent=4)}")
    return result

def create(args):
    """Create directive implementation"""
    if DATASET:
        raise Exception(f"create directive does not allow or use datasets")
    data,options,config = get_args(args)
    result = set_index(data)
    debug(f"create(args='{args}') --> data = {dataframe_to_table(result)}options = {json.dumps(options,indent=4)}\nconfig = {json.dumps(config,indent=4)}")
    return result

def set_distance(data):
    """Computes the distance and heading from latitude and longitude"""
    path = {CONFIG['column_names']['DIST']:[],CONFIG['column_names']['LAT']:[],CONFIG['column_names']['LON']:[],CONFIG['column_names']['HEAD']:[]}
    last = None
    post = 0.0
    global OPTIONS
    for pos in list(zip(data[CONFIG['column_names']['LAT']],data[CONFIG['column_names']['LON']])):
        segs = 1
        p0 = float(pos[0])
        p1 = float(pos[1])
        if last:
            dist = get_distance(pos,last)
            head = (270-numpy.arctan2(last[0]-p0,last[1]-p1)*180/math.pi)%360
            if OPTIONS['resolution'] and OPTIONS['resolution'] > 0 and OPTIONS['resolution'] < dist:
                segs = dist/OPTIONS['resolution']
                dlat = (p0-last[0])/segs
                dlon = (p1-last[1])/segs
                lat = last[0]
                lon = last[1]
        else:
            lat = p0
            lon = p1
            dist = 0.0
            head = 0.0
        while segs > 1:
            lat += dlat
            lon += dlon
            post += OPTIONS['resolution']
            path[CONFIG['column_names']['DIST']].append(round(post,OPTIONS['precision']["distance"]))
            path[CONFIG['column_names']['HEAD']].append(round(head,OPTIONS['precision']["heading"]))
            path[CONFIG['column_names']['LAT']].append(lat)
            path[CONFIG['column_names']['LON']].append(lon)
            dist -= OPTIONS['resolution']
            segs -= 1
        post += dist
        path[CONFIG['column_names']['DIST']].append(round(post,OPTIONS['precision']["distance"]))
        path[CONFIG['column_names']['HEAD']].append(round(head,OPTIONS['precision']["heading"]))
        path[CONFIG['column_names']['LAT']].append(pos[0])
        path[CONFIG['column_names']['LON']].append(pos[1])
        last = [p0,p1]
    return pandas.DataFrame(path)

def get_path(data):
    """Computes the path position index for path specified by latitude and longitude"""
    path = set_position(data)
    joinon = [CONFIG['column_names']['POS']]
    for other in [CONFIG['column_names']['LAT'],CONFIG['column_names']['LON'],CONFIG['column_names']['LOC'],CONFIG['column_names']['UUID']]:
        if other in data.columns:
            joinon.append(other)
    result = data.merge(path,how=CONFIG['path_join'])
    return set_position(result)

def get_distance(pos1,pos2):
    """Compute great circle distances between two points"""
    return haversine.haversine(pos1,pos2,unit=haversine.Unit.METERS)

def write_csv(data):
    """Write to CSV with headers"""
    specs = CONFIG['output_format'].split(":")
    if len(specs) == 1:
        data.to_csv(OUTPUT)
    else:
        try:
            data.to_csv(OUTPUT,columns=specs[1].split(","))
        except KeyError as err:
            error("write_csv(): invalid field specified",E_INVALID)

def filter(data):
    """Filter implementation"""
    if OPTIONS['filter']:
        for action in OPTIONS['filter'].split(';'):
            data = eval(f"data.{action}")
        if OPTIONS['select']:
            for column in OPTIONS['select'].split(','):
                data = data[data[column]]
        result = data
        debug(f"filter(data='{data}') --> {dataframe_to_table(result)}")
        return result
    else:
        return data

def write_raw(data):
    """Write raw output"""
    specs = CONFIG['output_format'].split(":")
    with open(OUTPUT,"w") as fh:
        if len(specs) == 1:
            print(OPTIONS['record_separator'].join(list(map(lambda x:OPTIONS['field_separator'].join(x.values()),data.astype('str').to_dict("records")))),file=fh)
        elif specs[1] == "FIELDS":
            print(OPTIONS['record_separator'].join(list(map(lambda x:OPTIONS['field_separator'].join(x.keys()),data.astype('str').to_dict("records")))),file=fh)
        else:
            try:
                print(OPTIONS['record_separator'].join(list(map(lambda x:OPTIONS['field_separator'].join(x.values()),data.astype('str')[specs[1].split(",")].to_dict("records")))),file=fh)
            except KeyError as err:
                error("write_raw(): specified field(s) not found in data",E_INVALID)

def write_pos(data):
    """Write lat,lon output"""
    data.to_csv(OUTPUT,columns=[CONFIG['column_names']['LAT'],CONFIG['column_names']['LON']],header=False,index=False,float_format=f"%.{OPTIONS['precision']['geolocation']}f")

def write_json(data):
    """Write JSON output"""
    global CONFIG
    specs = CONFIG['output_format'].split(":")
    global OUTPUT
    with open(OUTPUT,"w") as fh:
        global OPTIONS
        if len(specs) == 1:
            print(data.to_json(**OPTIONS['json']),file=fh)
        else:
            try:
                print(data[specs[1].split(",")].to_json(**OPTIONS['json']),file=fh)
            except KeyError as err:
                error("write_json(): invalid field specified",E_INVALID)

def write_glm(data):
    """Write GLM output"""
    specs = CONFIG['output_format'].split(":")
    if len(specs) == 1:
        columns = data.columns.to_list()
    else:
        columns = specs[1].split(",")
    modify = ( columns and columns[0][0] == '@' )
    if modify:
        columns[0] = columns[0][1:]
    else:
        if "class" not in data.columns:
            error("cannot write GLM objects without 'class' column in data frame",E_INVALID)
    with open(OUTPUT,"w") as fh:
        from datetime import datetime
        print(f"// generated by `{' '.join(sys.argv)}` at {datetime.now()}",file=fh)
        for id, row in data.iterrows():
            if not modify:
                print(f"object {row['class']}",file=fh)
                print("{",file=fh)
                if "name" in row.keys() and row["name"]:
                    print(f"\tname \"{row['name']}\";",file=fh)
                else:
                    print(f"\tname \"{row['class']}:{id}\";",file=fh)
            for prop, value in row.items():
                if prop in [CONFIG['column_names']['ID'],"class","name","id"]:
                    continue
                elif not columns or prop in columns:
                    if modify:
                        print(f"modify {row['name']}.{prop} \"{value}\";",file=fh)
                    else:
                        print(f"\t{prop} \"{value}\";",file=fh)
            if not modify:
                print("}",file=fh)

def write_table(data):
    specs = CONFIG['output_format'].split(":")
    pandas.set_option('display.max_rows', OPTIONS['table']['max_rows'])
    pandas.set_option('display.max_columns', OPTIONS['table']['max_columns'])
    pandas.set_option('display.width', OPTIONS['table']['width'])
    pandas.set_option('display.max_colwidth', OPTIONS['table']['max_colwidth'])
    if len(specs) == 1:
        print(data)
    elif specs[1][0] == '*':
        if len(specs[1]) > 1:
            print(data[specs[1][1:].split(",")])
        else:
            print(data)
    else:
        print(data[specs[1].split(",")])

def write_gdf(data):
    specs = CONFIG['output_format'].split(":")
    if len(specs) == 1:
        columns = data.columns
    else:
        columns = specs[1].split(",")

    if "geometry" not in data.columns:
        gdf = geopandas.GeoDataFrame(data[columns],geometry=geopandas.points_from_xy(data["longitude"], data["latitude"]))
    else:
        gdf = geopandas.GeoDataFrame(data[columns])
    gdf.to_file(OUTPUT)

def write_plot(data):
    specs = CONFIG['output_format'].split(":")
    if len(specs) == 1:
        specs.append("geometry")
    ax = None
    for field in specs[1].split(','):
        if field == 'geometry':
            if 'geometry' in data.columns:
                geodata = geopandas.GeoDataFrame(data,geometry=data['geometry'])
            else:
                error("geometry not in plot data",E_MISSING)
        elif field == 'location':
            if 'latitude' in data.columns and 'longitude' in data.columns:
                points = geopandas.points_from_xy(data['longitude'],data['latitude'])
                geodata = geopandas.GeoDataFrame(data,geometry=points)
            else:
                error("latitude/longitude not in plot data",E_MISSING)
        elif field == 'position':
            if 'latitude' in data.columns and 'longitude' in data.columns:
                points = geopandas.points_from_xy(data['longitude'],data['latitude'])
                error("unable to convert points to paths",E_INVALID)
                path = points # TODO
                geodata = geopandas.GeoDataFrame(data,geometry=path)
            else:
                error("position not in plot data",E_MISSING)
        else:
            error(f"PLOT field '{field}' is not valid",E_SYNTAX)
        if ax:
            geodata.plot(ax=ax,**OPTIONS['plot'])
        else:
            ax = geodata.plot(**OPTIONS['plot'])
    try:
        if OUTPUT == "/dev/stdout":
            plt.show(**OPTIONS['show'])
        else:
            plt.savefig(OUTPUT,**OPTIONS['savefig'])
    except Exception as err:
        error(f"write_plot(): {err}",E_INVALID)

def write_xlsx(data):
    global CONFIG
    specs = CONFIG['output_format'].split(":")
    global OUTPUT
    if len(specs) == 1:
        data.to_excel(OUTPUT)
    else:
        try:
            data[specs[1].split(",")].to_excel(OUTPUT)
        except KeyError as err:
            error("write_excel(): invalid field specified",E_INVALID)

def load_dataset(args):
    """Load the specified dataset implementation"""
    global DATASET
    global PKGDATA
    global MODULE
    DATASET=args[0]
    location = f"{PKGDATA}/geodata_{DATASET}.py"
    if not os.path.exists(location):
        error(f"dataset '{DATASET}' module is not found in '{location}'",E_NOTFOUND)
    verbose(f"dataset '{DATASET}' selected")
    modspec = util.spec_from_file_location(DATASET, location)
    global MODULE
    MODULE = importlib.import_module(f"geodata_{DATASET}")
    if not MODULE:
        error(f"unable to import module geodata_{DATASET} from {location}",E_NOTFOUND)
    if hasattr(MODULE,"set_context"):
        MODULE.set_context(sys.modules[__name__])
    return MODULE

def load_config(file=None,groups=[],inplace=False):
    global CONFIG
    global CONFIGDIR
    config = CONFIG
    if not file:
        if not groups:
            groups = CONFIGDIR.keys()
        for dir in groups:
            try:
                configfile = f'{CONFIGDIR[dir]}/{CONFIGFILE}'
                verbose(f"loading {configfile}")
                with open(configfile,'r') as fh:
                    config.update(json.load(fh))
            except:
                pass
    else:
        if file[0] == "+" and len(file) > 1:
            file = file[1:]
        elif file[0] != "+":
            config = {}
        if not os.path.exists(file):
            error(f"config file '{file}' does not exist",E_NOTFOUND)
        verbose(f"loading {file}")
        with open(file,'r') as fh:
            config.update(json.load(fh))
    if inplace:
        CONFIG = config
    return config

def set_config(name,value):
    specs = name.split(":")
    if len(specs) > 1:
        dir = specs[0]
        if not dir in CONFIGDIR.keys():
            raise Exception(f"{dir} configuration directory is invalid")
        name = ':'.join(specs[1:])
    else:
        name = specs[0]
        dir = 'local'
    configfile = f'{CONFIGDIR[dir]}/{CONFIGFILE}'
    try:
        with open(configfile,'r') as fh:
            config = json.load(fh)
    except:
        warning(f"config file '{configfile}' is not found")
        config = {}
    config[name] = value
    with open(configfile,'w') as fh:
        print(json.dumps(config),file=fh)

def unset_config(name):
    specs = name.split(":")
    if len(specs) > 1:
        dir = specs[0]
        if not dir in CONFIGDIR.keys():
            raise Exception(f"{dir} configuration directory is invalid")
        name = ':'.join(specs[1:])
    else:
        name = specs[0]
        dir = 'local'
    configfile = f'{CONFIGDIR[dir]}/{CONFIGFILE}'
    try:
        with open(configfile,'r') as fh:
            config = json.load(fh)
    except:
        warning(f"config file '{configfile}' is not found")
        config = {}
        pass
    try:
        del config[name]
    except:
        warning(f"config variable '{name}' not found in '{configfile}'")
        pass
    with open(configfile,'w') as fh:
        print(json.dumps(config),file=fh)

def get_config(name):
    return CONFIG[name]

def save_config(config=None):
    if config == None:
        config = CONFIG
    raise Exception(f"set_config(): not implemented")

def show_config(config={},groups=[],format="RAW"):
    if not config:
        config = load_config(groups)
    if format == "RAW":
        for tag, value in config.items():
            print(f"{tag}='{value}'")
    elif format == "JSON":
        print(json.dumps(config,indent=4),file=sys.stdout)
    elif format == "GLM":
        for tag, value in config.items():
            print(f"#define {tag}='{value}'")
    else:
        raise Exception(f"show_config(): format '{format}' is not valid")

def config(args):
    """Configuration directive implementation"""
    global CONFIG
    if args[0] == "show":
        if len(args) > 1:
            show_config(CONFIG,groups=args[1:])
        else:
            show_config(CONFIG)
    elif args[0] == "set":
        if len(args) > 2:
            set_config(*args[1:])
        else:
            error(f"config(args={args}): set missing arguments",E_MISSING)
    elif args[0] == "unset":
        if len(args) > 1:
            unset_config(*args[1:])
        else:
            error(f"config(args={args}): unset missing arguments",E_MISSING)
    elif args[0] == "get":
        if len(args) > 1:
            print(get_config(*args[1:]))
        else:
            error(f"config(args={args}): get missing argument",E_MISSING)
    else:
        error(f"config(args={args}): invalid config options",E_INVALID)

def main(argc,argv):
    global PKGDATA
    global DATASET
    global OUTPUT
    global OPTIONS
    n = 1
    DIRECTIVE = []
    if not PKGDATA or not os.path.exists(PKGDATA):
        warning("package folder '{PKGDATA}' not found, using default /usr/local/opt/gridlabd/share/gridlabd")
        PKGDATA = "/usr/local/opt/gridlabd/share/gridlabd"
    global CACHE
    CACHE = f"{PKGDATA}/geodata"
    load_config(inplace=True)
    while n < len(argv):
        if argv[n] in ["-c","-cache"]:
            n += 1
            CACHE = argv[n]
            if not os.path.exists(CACHE):
                warning(f"cache folder '{CACHE}' does not exist")
            else:
                verbose(f"using cache folder '{CACHE}'")
        elif argv[n] in ["-C","--configfile"]:
            n += 1
            if not os.path.exists(argv[n]):
                error(f"config file '{argv[n]}' not found",E_NOTFOUND)
            with open(argv[n],'r') as fh: update_values(OPTIONS,json.load(fh))
        elif argv[n] in ["--clear"]:
            import shutil
            shutil.rmtree(CACHE,ignore_errors=True)
            exit(E_OK)
        elif argv[n] in ["-d","--debug"]:
            OPTIONS['debug'] = True
            verbose("debug output enabled")

            import traceback
            import warnings
            import sys

            def warn_with_traceback(message, category, filename, lineno, file=None, line=None):

                log = file if hasattr(file,'write') else sys.stderr
                traceback.print_stack(file=log)
                warning(warnings.formatwarning(message, category, filename, lineno, line))

            warnings.showwarning = warn_with_traceback

        elif argv[n] in ["-D","--dataset"]:
            n += 1
            DATASET = argv[n]
            load_dataset([DATASET])
        elif argv[n] in ["-f","--format"]:
            n += 1
            if not argv[n].split(":")[0] in VALID_FORMATS:
                error(f"format '{argv[n]}' is not recognized",E_SYNTAX)
            CONFIG['output_format'] = argv[n]
            verbose(f"output format is set to '{CONFIG['output_format']}'")
        elif argv[n] in ["-h","--help"]:
            help()
            exit(E_OK)
        elif argv[n] in ["-j","--join"]:
            n += 1
            if not argv[n] in ["inner","outer","left","right"]:
                error(f"path join '{argv[n]}' is not valid",E_SYNTAX)
            CONFIG['path_join'] = argv[n]
            verbose(f"path join is set to '{CONFIG['path_join']}'")
        elif argv[n] in ["-k","--key"]:
            n += 1
            OPTIONS['key_index'] = argv[n]
            verbose(f"key index is set to '{OPTIONS['key_index']}'")
        elif argv[n] in ["-o","--output"]:
            global OUTPUT
            n += 1
            OUTPUT = argv[n]
            verbose(f"output is set to '{OUTPUT}'")
        elif argv[n] in ["-p","--precision"]:
            n += 1
            if not set_value(f"precision.geolocation={argv[n]}",OPTIONS):
                error(f"precision '{argv[n]}' is not a valid value",E_INVALID)
            if OPTIONS['precision']["geolocation"] < 0:
                error(f"geoprecision must be zero or positive integer",E_INVALID)
            verbose(f"geoprecision is set to '{OPTIONS['precision']['geolocation']}'")
        elif argv[n] in ["-q","--quiet"]:
            OPTIONS['quiet'] = True
            verbose("quiet output enabled")
        elif argv[n] in ["-r","--resolution"]:
            n += 1
            OPTIONS['resolution'] = float(argv[n])
            verbose("path resolution is '{OPTIONS['resolution']}' meters")
        elif argv[n] in ["-s","--silent"]:
            OPTIONS['silent'] = True
            verbose("silent output enabled")
        elif argv[n] in ["-T","--threadcount"]:
            n += 1
            OPTIONS['maxthreads'] = int(argv[n])
            if OPTIONS['maxthreads'] < 0:
                error("maximum thread count must non-negative",E_INVALID)
            verbose(f"maximum thread count '{OPTIONS['maxthreads']}'")
        elif argv[n] in ["-v","--verbose"]:
            OPTIONS['verbose'] = True
            verbose("verbose output enabled")
        elif argv[n] in ["-w","--warning"]:
            OPTIONS['warning'] = False
            verbose("warning output disable")
        elif argv[n] == "--fieldsep":
            n += 1
            OPTIONS['field_separator'] = argv[n]
            verbose("field separate is '{OPTIONS['field_separator']}'")
        elif argv[n] == "--recordsep":
            n += 1
            OPTIONS['record_separator'] = argv[n]
            verbose("record separate is '{OPTIONS['record_separator']}'")
        elif argv[n] == "--show_options":
            show_values(OPTIONS)
            exit(E_OK)
        elif argv[n] == "--show_config":
            show_values(CONFIG)
            exit(E_OK)
        else:
            DIRECTIVE.append(argv[n])
        n += 1
    if len(DIRECTIVE) == 0:
        syntax()
        exit(E_SYNTAX)
    elif len(DIRECTIVE) == 1:
        args = []
    else:
        args = DIRECTIVE[1:]
    if DIRECTIVE[0] == "help":
        help(args)
        exit(E_OK)
    elif not DIRECTIVE[0] in DIRECTIVES:
        error(f"directive '{DIRECTIVE[0]}' is not valid",E_SYNTAX)
    data = globals()[DIRECTIVE[0]](args)
    if type(data) is pandas.DataFrame:
        data = filter(data)
        if CONFIG['output_format'] == "CSV" or CONFIG['output_format'].startswith("CSV:"):
            write_csv(data)
        elif CONFIG['output_format'] == "RAW" or CONFIG['output_format'].startswith("RAW:"):
            write_raw(data)
        elif CONFIG['output_format'] == "JSON" or CONFIG['output_format'].startswith("JSON:"):
            write_json(data)
        elif CONFIG['output_format'] == "GLM" or CONFIG['output_format'].startswith("GLM:"):
            write_glm(data)
        elif CONFIG['output_format'] == "POS":
            write_pos(data)
        elif CONFIG['output_format'] == "TABLE" or CONFIG['output_format'].startswith("TABLE:"):
            write_table(data)
        elif CONFIG['output_format'] == "PLOT" or CONFIG['output_format'].startswith("PLOT:"):
            write_plot(data)
        elif CONFIG['output_format'] == "GDF" or CONFIG['output_format'].startswith("GDF:"):
            write_gdf(data)
        elif CONFIG['output_format'] == "XLSX" or CONFIG['output_format'].startswith("XLSX:"):
            write_xlsx(data)
        else:
            error(f"format '{CONFIG['output_format']}' is invalid",E_SYNTAX)

if __name__ == "__main__":
    main(len(sys.argv),sys.argv)
    if warning_count > 1:
        warning(f"last warning occurred {warning_count} times")
