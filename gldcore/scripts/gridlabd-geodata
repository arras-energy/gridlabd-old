#!/usr/local/bin/python3
"""Syntax: gridlabd geodata OPTIONS DIRECTIVE [ARGUMENTS]

The geodata command gathers and joins geographic data. The geodata subcommand
uses directives that are documented in the DIRECTIVES section below.

In general geodata is used to acquire geographic information at a location or
along a specified path. This information includes ground elevation, vegetation
characteristics, weather, census, building, and transportation data.  The
specific data sets and their origins are described in the DATASETS section
below.

OPTIONS

  [-c|--cache FOLDER]         change cache folder
  [-C|--config FILE]          change config file
  [-d|--debug]                enable debug output, including error traceback
  [-f|--format FORMAT]        change output format
  [-j|--join TYPE]            control how dataset joins with input path
  [-k|--key KEY]              change method for generating keys
  [-o|--output CSVOUT]        output to CSVOUT
  [--precision=INT]           change precision of distance measurement
  [-q|--quiet]                disable normal output
  [-r|--resolution=METERS]       change the resolution at which positions are merged
  [-s|--silent]               disable error output
  [-T|--threadcount THREADS]  change maximum thread count
  [--units UNIT]              change units of distance
  [-v|--verbose]              enable verbose output
  [-w|--warning]              disable warning output

REMARKS

The default cache folder is /usr/local/share/gridlabd/geodata. The default
config file $HOME/.gridlabd/geodata-config. The default maximum thread count
is 1.

DIRECTIVES

The following directives are available:

  config get PARAMETER config set PARAMETER VALUE config show [PATTERN]

    The config directive changes values in the config file.

  create [ARGUMENTS]

    The create directive generates a new data from the specified
    dataset.

  merge [-D|--dataset] NAME [ARGUMENTS]

    The merge diretive merges the results from the specified dataset with the
    data.  If a column exists, it is updated. If a column does not exist, it is
    created.

    The -D|--dataset directive specifies the dataset from which the geographic
    information is to be acquired. See the DATASETS section below for details.

ARGUMENTS

All directives support the following arguments

  <name>=<value>[+<name>=<value>[+...]]

    Specifies one or more name/value tuples from which to construct a dataframe.
    Note that by default all fields are string.  Use CSV format inputs to use
    automatic types.

  CSVIN [...]

    Specifies one or more files from which CSV input is read. If no values or
    CSV input files are specified, the CSV data input is read from /dev/stdin.

  [-o|--output] CSVOUT

    Normally the output is written to /dev/stdout. When CSVOUT is specified the
    output data is written to the specified file. If the output file already
    exists, it is overwritten with the new data.

KEYS

The [-k|--key KEY] options changes how row keys are generated. The default is
no key, in which case there is no guarantee that the rows will be output in the
same order in which they were input.

  [-k|--key "location"]

    The "location" key computes geohash codes. Rows are indexed on the key named
    "location".

  [-k|--key "position"]

    The "position" key performs a distance calculation with respect to the first
    entry in the data, thus maintaining the ordering the rows.  Rows are indexed
    on the key names "position".

If the [-r|--resolution METERS] option is used, the keys are generated at
the specified resolution along the path. The default resolution is None. If
resolution is specified without a key, then "position" is used.

DATASETS

The following datasets are available. To specify a layer, use the syntax
"NAME.LAYER".  Multiple datasets and layers may be specified using a
comma-delimited list.

  building

    The building dataset provides build environment data from NREL. Available
    layers are "building_type" and "building_size".

  census

    The census dataset provides economic and population data from the US Census
    Bureau.

  distance

    The distance dataset calculates the distance between each point and the
    point before it.

  elevation

    The elevation dataset provides 1 arcsecond resolution ground elevation data
    from the USGS.

  powerline

    The powerline dataset provides powerline geometry calculations based on
    the GridLAB-D cable library.

  transportation

    The transportation dataset provides mobility data from traffic data services.
    You must subscribe to the TrafficView service to use this dataset.

  utility

    The utility data provides data about the utility servicing the locations.

  vegetation

    The vegetation dataset provides 7 layers of data about vegetation.  Available
    layers are "canopy_cover", "canopy_height", "base_height", "bulk_density",
    "layer_count", "fuel_density", and "surface_fuels".. You must subscribe to the
    Forest Observatory service to use this dataset.

  weather

    The weather dataset provides historical, current, and short term forecasts of
    dry bulk, wet bulb, solar, and wind data from NOAA.

FORMATS

Output is generated by default in CSV format.  Other output formats may be
selected as follows:

  CSV[:<fields>]    fields are output in CSV format

  JSON[:<fields>]   fields are output in JSON format

  RAW[:<fields>]    fields are output as a raw string

  GLM[:[@]<fields>] fields are output as GLM objects. If @ included, output is
                    is GLM modify statements. Note that invalid fields name do
                    not raise a KeyError exception or error message. This is
                    because CSV tables may contain multiple classes with some
                    columns that are not defined for every class.

DEVELOPER API

Packages must implement the following (TODO indicates where your code goes)

    version = 1

    default_options = {
        TODO,
    }

    default_config = {
        TODO,
    }

    def apply(data, options=default_options, config=default_config):
        result = TODO
        return pandas.DataFrame(result)

    if __name__ == '__main__':
        if len(sys.argv) == 1 or sys.argv[1] in ["-h","--help","help"]:
            print(f"Syntax: {sys.argv[0].split('/')[-1]} [unittest|makeconfig]")
        elif sys.argv[1] in ["unittest"]:
            import unittest
            class TestAddress(unittest.TestCase):
                def test_1(self):
                    TODO
            unittest.main()
        elif sys.argv[1] in ["makeconfig"]:
            with open(sys.argv[0].replace(".py",".cfg"),"w") as fh:
                json.dump(default_config,fh,indent=4)
        else:
            raise Exception(f"'{sys.argv[0]}' is an invalid command option")

See geodata_distance.py for a simple example of a dataset implementation.

EXAMPLES

1) Creating dataset and output formats

    To create a new dataset from the command line and output as CSV:

        $ gridlabd geodata create name=addr1+latitude=37.4150+longitude=-122.2056+class=example \
        > name=addr2+latitude=37.3880+longitude=-122.2884+class=example
        id,name,latitude,longitude,class
        0,addr1,37.4150,-122.2056,example
        1,addr2,37.3880,-122.2884,example

    To output a single field:

        $ gridlabd geodata create name=addr1+latitude=37.4150+longitude=-122.2056+class=example \
        > name=addr2+latitude=37.3880+longitude=-122.2884+class=example -f CSV:name
        id,name
        0,addr1
        1,addr2

    To output multiple fields:

        $ gridlabd geodata create name=addr1+latitude=37.4150+longitude=-122.2056+class=example \
        > name=addr2+latitude=37.3880+longitude=-122.2884+class=example -f CSV:name,class
        id,name,class
        0,addr1,example
        1,addr2,example

    Create the same data, but output JSON

        $ gridlabd geodata create name=addr1+latitude=37.4150+longitude=-122.2056+class=example \
        > name=addr2+latitude=37.3880+longitude=-122.2884+class=example -f JSON
        [{"name":"addr1","latitude":"37.4150","longitude":"-122.2056","class":"example"},{"name":"addr2","latitude":"37.3880","longitude":"-122.2884","class":"example"}]

    Same data, but output as GLM

        $ gridlabd geodata create name=addr1+latitude=37.4150+longitude=-122.2056+class=example \
        > name=addr2+latitude=37.3880+longitude=-122.2884+class=example -f GLM
        // generated by `/usr/local/opt/gridlabd/4.2.20-210422-develop_geodata_subcommand/bin/gridlabd-geodata create name=addr1+latitude=37.4150+longitude=-122.2056+class=example name=addr2+latitude=37.3880+longitude=-122.2884+class=example -f GLM` at 2021-04-22 08:38:37.163874
        object example
        {
        	name "{row['name']}";
        	latitude "37.4150";
        	longitude "-122.2056";
        }
        object example
        {
        	name "{row['name']}";
        	latitude "37.3880";
        	longitude "-122.2884";
        }

    Same data, but output GLM modify statements

        $ gridlabd geodata create name=addr1+latitude=37.4150+longitude=-122.2056+class=example \
        > name=addr2+latitude=37.3880+longitude=-122.2884+class=example -f GLM:@latitude,longitude
        // generated by `/usr/local/opt/gridlabd/4.2.20-210422-develop_geodata_subcommand/bin/gridlabd-geodata create name=addr1+latitude=37.4150+longitude=-122.2056+class=example name=addr2+latitude=37.3880+longitude=-122.2884+class=example -f GLM:@latitude,longitude` at 2021-04-22 08:38:49.759341
        modify addr1.longitude "-122.2056";
        modify addr2.longitude "-122.2884";

2) Merging a geographic dataset into an exist dataframe

The following command uses to address dataset on locations provided on the command line:

    $ gridlabd geodata merge -D address name=addr1+latitude=37.4150+longitude=-122.2056+class=example \
    > name=addr2+latitude=37.3880+longitude=-122.2884+class=example
    id,name,latitude,longitude,class,address
    0,addr1,37.4150,-122.2056,example,"Stanford Linear Accelerator Center National Accelerator Laboratory, Sand Hill Road, Menlo Park, San Mateo County, California, 94028, United States"
    1,addr2,37.3880,-122.2884,example,"Allen Road, San Mateo County, California, United States"

The following command merges addresses into an existing dataframe

    $gridlabd geodata create name=addr1+latitude=37.4150+longitude=-122.2056+class=example \
    > name=addr2+latitude=37.3880+longitude=-122.2884+class=example | gridlabd geodata merge -D address
    id,name,latitude,longitude,class,address
    0,addr1,37.415,-122.2056,example,"Stanford Linear Accelerator Center National Accelerator Laboratory, Sand Hill Road, Menlo Park, San Mateo County, California, 94028, United States"
    1,addr2,37.388,-122.2884,example,"Allen Road, San Mateo County, California, United States"

3) GLM usage

The following example uses the geodata subcommand to modify GridLAB-D objects with
data acquired from the address dataset.

    clock {
        starttime "2020-01-01 00:00:00";
        stoptime "2021-01-01 00:00:00";
    }
    module residential;
    class house
    {
        char1024 address;
    }

    object house
    {
        address "2575 Sand Hill Rd., Menlo Park, CA";
    }

    object house
    {
        address "2000 Broadway, Redwood City, CA";
    }

    // write house.address to csv
    #write /tmp/write.csv house:address

    // get lat,lon from address and write to csv
    #geodata merge -D address /tmp/write.csv --reverse --format GLM:latitude,longitude -o /tmp/read.glm

    // read csv as modify
    #include "/tmp/read.glm"

    #set savefile="/tmp/final.json"

After this GridLAB-D model is loaded and run, the output json file can be read using the subcommand, e.g.,

    $ gridlabd json-get objects house:0 latitude </tmp/final.json
    37.415463

"""
import sys, os
import importlib
from importlib import util
import json
import math, numpy
import pandas

NAME = "geodata"
GEODATA = sys.modules[__name__]
DIRECTIVES = ["config","create","merge"]
VERBOSE = False
DEBUG = False
SILENT = False
QUIET = False
DEBUG = False
CFGFILE = f"os.getenv('HOME')/.gridlabd/geodata.conf"
GEODATAURL = "http://geodata.gridlabd.us/"
MAXTHREADS = 1
DATASET = None
OUTPUT = "/dev/stdout"
INPUT = "/dev/stdin"
MODULE = None
CONFIGDATA = {}
PKGDATA = os.getenv("GLD_ETC")
if not PKGDATA:
    PKGDATA = "/usr/local/share/gridlabd"
FORMAT = "CSV"
PATHJOIN = "outer"
CONFIGDIR = {
    "system" : PKGDATA + "/geodata",
    "user" : os.getenv("HOME") + "/.gridlabd/geodata",
    "local" : os.getenv("PWD"),
}
KEYINDEX = None
DEFAULTOPTIONS = {
}
RESOLUTION = None
IDPRECISION = 0
GEOPRECISION = 4
OPTIONS = {}
CONFIGURATION = {}
FIELDSEP = ' '
RECORDSEP = '\n'

E_OK = 0
E_SYNTAX = 1
E_NOTFOUND = 2

def error(msg,exitcode=None):
    text = f"ERROR [{NAME}/{DATASET}]: {msg}"
    if DEBUG:
        raise Exception(msg)
    if not SILENT:
        print(text,file=sys.stderr,flush=True)
    if exitcode:
        exit(exitcode)

def warning(msg):
    if not QUIET:
        text = f"WARNING [{NAME}/{DATASET}]: {msg}"
        print(text,file=sys.stderr)

def verbose(msg):
    if VERBOSE:
        text = f"VERBOSE [{NAME}/{DATASET}]: {msg}"
        print(text,file=sys.stderr)

def output(msg):
    if not QUIET:
        print(msg,file=sys.stdout)

def debug(msg):
    if DEBUG:
        print(f"DEBUG [{NAME}/{DATASET}]: {msg}", file=sys.stderr)

python_help = help
def help(dataset=None):
    if dataset:
        MODULE = load_dataset(dataset)
        if MODULE:
            python_help(MODULE.__name__)
        else:
            error(f"command '{command}' not found",E_SYNTAX)
    else:
        python_help(__name__)
    return None, None

def syntax():
    print(__doc__.split("\n")[0],file=sys.stdout)

def dataframe_to_table(data):
    table = f"{data}"
    rule = "-"*max(map(lambda x:len(x),table.split("\n")))
    return f"\n{rule}\n{table}\n{rule}\n"

#
# GEOHASH support
#
from math import log10

#  Note: the alphabet in geohash differs from the common base32
#  alphabet described in IETF's RFC 4648
#  (http://tools.ietf.org/html/rfc4648)
__base32 = '0123456789bcdefghjkmnpqrstuvwxyz'
__decodemap = { }
for i in range(len(__base32)):
    __decodemap[__base32[i]] = i
del i

def decode_exactly(geohash):
    """
    Decode the geohash to its exact values, including the error
    margins of the result.  Returns four float values: latitude,
    longitude, the plus/minus error for latitude (as a positive
    number) and the plus/minus error for longitude (as a positive
    number).
    """
    lat_interval, lon_interval = (-90.0, 90.0), (-180.0, 180.0)
    lat_err, lon_err = 90.0, 180.0
    is_even = True
    for c in geohash:
        cd = __decodemap[c]
        for mask in [16, 8, 4, 2, 1]:
            if is_even: # adds longitude info
                lon_err /= 2
                if cd & mask:
                    lon_interval = ((lon_interval[0]+lon_interval[1])/2, lon_interval[1])
                else:
                    lon_interval = (lon_interval[0], (lon_interval[0]+lon_interval[1])/2)
            else:      # adds latitude info
                lat_err /= 2
                if cd & mask:
                    lat_interval = ((lat_interval[0]+lat_interval[1])/2, lat_interval[1])
                else:
                    lat_interval = (lat_interval[0], (lat_interval[0]+lat_interval[1])/2)
            is_even = not is_even
    lat = (lat_interval[0] + lat_interval[1]) / 2
    lon = (lon_interval[0] + lon_interval[1]) / 2
    return lat, lon, lat_err, lon_err

def decode(geohash):
    """
    Decode geohash, returning two strings with latitude and longitude
    containing only relevant digits and with trailing zeroes removed.
    """
    lat, lon, lat_err, lon_err = decode_exactly(geohash)
    # Format to the number of decimals that are known
    lats = "%.*f" % (max(1, int(round(-log10(lat_err)))) - 1, lat)
    lons = "%.*f" % (max(1, int(round(-log10(lon_err)))) - 1, lon)
    if '.' in lats: lats = lats.rstrip('0')
    if '.' in lons: lons = lons.rstrip('0')
    return lats, lons

def encode(latitude, longitude, precision=12):
    """
    Encode a position given in float arguments latitude, longitude to
    a geohash which will have the character count precision.
    """
    lat_interval, lon_interval = (-90.0, 90.0), (-180.0, 180.0)
    geohash = []
    bits = [ 16, 8, 4, 2, 1 ]
    bit = 0
    ch = 0
    even = True
    while len(geohash) < precision:
        if even:
            mid = (lon_interval[0] + lon_interval[1]) / 2
            if longitude > mid:
                ch |= bits[bit]
                lon_interval = (mid, lon_interval[1])
            else:
                lon_interval = (lon_interval[0], mid)
        else:
            mid = (lat_interval[0] + lat_interval[1]) / 2
            if latitude > mid:
                ch |= bits[bit]
                lat_interval = (mid, lat_interval[1])
            else:
                lat_interval = (lat_interval[0], mid)
        even = not even
        if bit < 4:
            bit += 1
        else:
            geohash += __base32[ch]
            bit = 0
            ch = 0
    return ''.join(geohash)

def get_latlon(pos):
    """Convert a lat,lon string to a float pair"""
    try:
        latlon = pos.split(",")
        lat = float(latlon[0])
        lon = float(latlon[1])
        if -90 <= lat <= 90 and -180 <= lon <= 180:
            # debug(f"get_latlon(pos={pos}) --> {lat},{lon}")
            return lat,lon
        else:
            return None, None
    except:
        return None, None

def set_location(data):
    """Update the location index on the data"""
    try:
        result = pandas.DataFrame(data)
        result["location"] = list(map(lambda pos: encode(float(pos[0]),float(pos[1])),zip(result["latitude"],result["longitude"])))
        result.set_index("location",inplace=True)
    except:
        result = data
    # debug(f"set_position(data={dataframe_to_table(data)} --> {dataframe_to_table(result)}")
    return result

def set_position(data):
    """Update the path position index from the distance column"""
    if "distance" not in data.columns:
        result = pandas.DataFrame(data).merge(set_distance(data),how="outer")
    else:
        result = pandas.DataFrame(data)
    result["position"] = pandas.Series(result["distance"]*10**IDPRECISION,dtype="int32")
    result = result.set_index("position").sort_index()
    # debug(f"set_position(data={dataframe_to_table(data)} --> {dataframe_to_table(result)}")
    return result

def set_index(data,index=None,silent=False):
    """Set the specified index on the data"""
    if index is None:
        index = KEYINDEX
    # debug(f"set_index(data={dataframe_to_table(data)},index='{index}',silent={silent})")
    if index is None:
        if data.index.name:
            data.reset_index(inplace=True)
        if "id" in data.columns:
            data.set_index("id",inplace=True)
        else:
            data.index.name = "id"
        return data
    elif data.index.name == index:
        return data
    elif index == "location":
        return set_location(data)
    elif index == "position":
        return set_position(data)
    elif not type(index) is type(None):
        columns = index.split(",")
        try:
            return data.set_index(columns)
        except:
            if not silent:
                warning(f"unable to set index to '{index}'")
            return data
    elif index in data.columns:
        return data.set_index(index)
    else:
        error("index '{index}' is not valid")

def get_args(args):
    """Get the arguments, options, and configuration"""
    data = []
    options = DEFAULTOPTIONS
    config = CONFIGDATA
    if MODULE:
        options.update(MODULE.default_options)
        config.update(MODULE.default_config)
    for arg in args:
        lat,lon = get_latlon(arg)
        if arg[0:2] == "--" and len(arg) > 2:
            opts = arg[2:].split("=")
        else:
            opts = [None]
        if opts[0] in config.keys():
            if len(opts) == 1:
                config[opts[0]] = not config[opts[0]]
            else:
                config[opts[0]] = type(config[opts[0]])(opts[1])
        elif opts[0] in options.keys():
            if len(opts) == 1:
                options[opts[0]] = not options[opts[0]]
            else:
                options[opts[0]] = type(options[opts[0]])(opts[1])
        elif type(lat) == float and type(lon) == float:
            data.append(pandas.DataFrame({"latitude":[lat],"longitude":[lon]}))
        elif os.path.exists(arg):
            data.append(pandas.read_csv(arg))
        else:
            items = arg.split("+")
            values = {}
            for item in items:
                specs = item.split("=")
                if len(specs) == 1:
                    if DATASET is None:
                        error(f"'{arg}' is not recognized without a dataset ",E_SYNTAX)
                    else:
                        values[DATASET] = [specs[0]]
                else:
                    values[specs[0]] = [specs[1]]
            data.append(pandas.DataFrame(values))
    if not data:
        data = pandas.read_csv("/dev/stdin")
    else:
        data = pandas.concat(data,ignore_index=True)

    global RESOLUTION
    global KEYINDEX
    if not KEYINDEX and RESOLUTION:
        KEYINDEX = "position"
    if KEYINDEX in ["position","location"]:
        data = set_index(data)
    # debug(f"get_args(args={args}) -->{dataframe_to_table(data)}options = {options}\nconfig = {config}")
    OPTIONS = options
    CONFIGURATION = config
    return data,options,config

def merge(args):
    """Merge directive implementation"""
    data,options,config = get_args(args)
    # debug(f"{DATASET}.apply(data={dataframe_to_table(data)}options = {options}\nconfig = {config})")
    try:
        result = MODULE.apply(data,options,config)
    except Exception as err:
        result = err
    if type(result) is Exception:
        if DEBUG:
            import traceback
            print("Traceback (most recent call last):")
            traceback.print_tb(result.__traceback__)
            print(f"Exception: {str(result)}")
        else:
            error(str(result))
        result = None
    else:
        result = set_index(result)
    debug(f"merge(args={dataframe_to_table(data)}) -->\ndata = {dataframe_to_table(data)}options = {options}\nconfig = {config}")
    return result

def create(args):
    """Create directive implementation"""
    if DATASET:
        raise Exception(f"create directive does not allow or use datasets")
    data,options,config = get_args(args)
    result = set_index(data)
    debug(f"create(args={dataframe_to_table(data)}) -->\ndata = {dataframe_to_table(data)}options = {options}\nconfig = {config}")
    return result

def set_distance(data):
    """Computes the distance and heading from latitude and longitude"""
    path = {"distance":[],"latitude":[],"longitude":[],"heading":[]}
    last = None
    post = 0.0
    global RESOLUTION
    for pos in list(zip(data["latitude"],data["longitude"])):
        segs = 1
        if last:
            dist = get_distance(pos,last)
            head = (270-numpy.arctan2(last[0]-pos[0],last[1]-pos[1])*180/math.pi)%360
            if RESOLUTION and RESOLUTION > 0 and RESOLUTION < dist:
                segs = dist/RESOLUTION
            dlat = (float(pos[0])-last[0])/segs
            dlon = (float(pos[1])-last[1])/segs
        else:
            lat = float(pos[0])
            lon = float(pos[1])
            dist = 0.0
            head = float('nan')
        while segs > 1:
            lat += dlat
            lon += dlon
            post += RESOLUTION
            path["distance"].append(post)
            path["heading"].append(head)
            path["latitude"].append(lat)
            path["longitude"].append(lon)
            dist -= RESOLUTION
            segs -= 1
        post += dist
        path["distance"].append(post)
        path["heading"].append(head)
        path["latitude"].append(pos[0])
        path["longitude"].append(pos[1])
        last = pos
    return pandas.DataFrame(path)

def get_path(data):
    """Computes the path position index for path specified by latitude and longitude"""
    path = set_position(data)
    joinon = ["position"]
    for other in ["latitude","longitude","location"]:
        if other in data.columns:
            joinon.append(other)
    # debug(f"joinon = {joinon}")
    # debug(f"data =\n{dataframe_to_table(data)}")
    # debug(f"path =\n{dataframe_to_table(path)}")
    #result = data.reset_index().join(path.reset_index(),on=joinon,how=PATHJOIN,sort=True)
    result = data.merge(path,how=PATHJOIN)
    # debug(f"result =\n{dataframe_to_table(result)}")
    return set_position(result)

def get_distance(pos1,pos2):
    """Compute haversine distances along a path
    ARGUMENTS

        path ((lat,lon) tuple) Specifies the path along which the distance is
                            computed

    RETURNS

        list   The cumulative distances along the path
    """
    lat1 = pos1[0]*math.pi/180
    lat2 = pos2[0]*math.pi/180
    lon1 = pos1[1]*math.pi/180
    lon2 = pos2[1]*math.pi/180
    a = math.sin((lat2-lat1)/2)**2+math.cos(lat1)*math.cos(lat2)*math.sin((lon2-lon1)/2)**2
    return 6371e3*(2*numpy.arctan2(numpy.sqrt(a),numpy.sqrt(1-a)))

def write_csv(data):
    """Write to CSV with headers"""
    specs = FORMAT.split(":")
    if len(specs) == 1:
        data.to_csv(OUTPUT)
    else:
        try:
            data.to_csv(OUTPUT,columns=specs[1].split(","))
        except KeyError as err:
            error("write_csv(): invalid field specified")

def write_raw(data):
    """Write raw output"""
    specs = FORMAT.split(":")
    with open(OUTPUT,"w") as fh:
        if len(specs) == 1:
            print(RECORDSEP.join(list(map(lambda x:FIELDSEP.join(x.values()),data.astype('str').to_dict("records")))),file=fh)
        elif specs[1] == "FIELDS":
            print(RECORDSEP.join(list(map(lambda x:FIELDSEP.join(x.keys()),data.astype('str').to_dict("records")))),file=fh)
        else:
            try:
                print(RECORDSEP.join(list(map(lambda x:FIELDSEP.join(x.values()),data.astype('str')[specs[1].split(",")].to_dict("records")))),file=fh)
            except KeyError as err:
                error("write_raw(): specified field(s) not found in data")

def write_pos(data):
    """Write lat,lon output"""
    data.to_csv(OUTPUT,columns=["latitude","longitude"],header=False,index=False,float_format=f"%.{GEOPRECISION}f")

def write_json(data):
    """Write JSON output"""
    specs = FORMAT.split(":")
    with open(OUTPUT,"w") as fh:
        if len(specs) == 1:
            print(data.to_json(orient="records"),file=fh)
        else:
            try:
                print(data[specs[1].split(",")].to_json(orient="records"),file=fh)
            except KeyError as err:
                error("write_json(): invalid field specified")

def write_glm(data):
    """Write GLM output"""
    specs = FORMAT.split(":")
    if len(specs) == 1:
        columns = data.columns.to_list()
    else:
        columns = specs[1].split(",")
    modify = ( columns and columns[0][0] == '@' )
    if modify:
        columns[0] = columns[0][1:]
    else:
        if "class" not in data.columns:
            error("cannot write GLM objects without 'class' column in data frame")
    if "name" not in data.columns:
        error("cannot write GLM without 'name' column in data frame")
    with open(OUTPUT,"w") as fh:
        from datetime import datetime
        print(f"// generated by `{' '.join(sys.argv)}` at {datetime.now()}",file=fh)
        for id, row in data.iterrows():
            if not modify:
                print(f"object {row['class']}",file=fh)
                print("{",file=fh)
                print("\tname \"{row['name']}\";",file=fh)
            for prop, value in row.items():
                if prop in ["id","class","name"]:
                    continue
                elif not columns or prop in columns:
                    if modify:
                        print(f"modify {row['name']}.{prop} \"{value}\";",file=fh)
                    else:
                        print(f"\t{prop} \"{value}\";",file=fh)
            if not modify:
                print("}",file=fh)

def load_dataset(args):
    """Load the specified dataset implementation"""
    global DATASET
    global PKGDATA
    global MODULE
    DATASET=args[0]
    location = f"{PKGDATA}/geodata_{DATASET}.py"
    if not os.path.exists(location):
        error(f"dataset '{DATASET}' module is not found in '{location}'",E_NOTFOUND)
    verbose(f"dataset '{DATASET}' selected")
    modspec = util.spec_from_file_location(DATASET, location)
    global MODULE
    MODULE = importlib.import_module(f"geodata_{DATASET}")
    if not MODULE:
        error(f"unable to import module geodata_{DATASET} from {location}",E_NOTFOUND)
    if hasattr(MODULE,"set_context"):
        MODULE.set_context(sys.modules[__name__])
    return MODULE

def load_config():
    return DEFAULT
def config(args):
    """Configuration directive implementation"""
    global CONFIGURATION
    if args[0] == "show":
        print(CONFIGURATION)
    else:
        error(f"config(args={args}): invalid config options")

def main(argc,argv):
    global PKGDATA
    global DATASET
    global FORMAT
    global KEYINDEX
    global PATHJOIN
    global CFGFILE
    global OUTPUT
    n = 1
    DIRECTIVE = []
    if not PKGDATA or not os.path.exists(PKGDATA):
        warning("package folder '{PKGDATA}' not found, using default /usr/local/share/gridlabd")
        PKGDATA = "/usr/local/share/gridlabd"
    while n < len(argv):
        if argv[n] in ["-c","-cache"]:
            global CACHE
            n += 1
            CACHE = argv[n]
            if not os.path.exists(CACHE):
                warning(f"cache folder '{CACHE}' does not exist")
            else:
                verbose(f"using cache folder '{CACHE}'")
        elif argv[n] in ["-C","--configfile"]:
            global CFGFILE
            n += 1
            CFGFILE = argv[n]
            if not os.path.exists(CFGFILE):
                warning(f"config file '{CFGFILE}' does not exist")
            else:
                verbose(f"using GLM file {GLMFILE}")
        elif argv[n] in ["-d","--debug"]:
            global DEBUG
            DEBUG = True
            verbose("debug output enabled")
        elif argv[n] in ["-D","--dataset"]:
            n += 1
            DATASET = argv[n]
            load_dataset([DATASET])
        elif argv[n] in ["-f","--format"]:
            n += 1
            if not argv[n].split(":")[0] in ["CSV","RAW","JSON","POS","GLM","FIELD","POS"]:
                error(f"format '{argv[n]}' is not recognized",E_SYNTAX)
            FORMAT = argv[n]
            verbose(f"output format is set to '{FORMAT}'")
        elif argv[n] in ["-h","--help"]:
            help()
            exit(E_OK)
        elif argv[n] in ["-j","--join"]:
            global PATHJOIN
            n += 1
            if not argv[n] in ["inner","outer","left","right"]:
                error(f"path join '{argv[n]}' is not valid",E_SYNTAX)
            PATHJOIN = argv[n]
            verbose(f"path join is set to '{PATHJOIN}'")
        elif argv[n] in ["-k","--key"]:
            global KEYINDEX
            n += 1
            KEYINDEX = argv[n]
            verbose(f"key index is set to '{KEYINDEX}'")
        elif argv[n] in ["-o","--output"]:
            global OUTPUT
            n += 1
            OUTPUT = argv[n]
            verbose(f"output is set to '{OUTPUT}'")
        elif argv[n] in ["-p","--precision"]:
            global GEOPRECISION
            n += 1
            GEOPRECISION = argv[n]
            verbose(f"geoprecision is set to '{GEOPRECISION}'")
        elif argv[n] in ["-q","--quiet"]:
            global QUIET
            QUIET = True
            verbose("quiet output enabled")
        elif argv[n] in ["-r","--resolution"]:
            global RESOLUTION
            n += 1
            RESOLUTION = float(argv[n])
            verbose("path resolution is '{RESOLUTION}' meters")
        elif argv[n] in ["-s","--silent"]:
            global SILENT
            SILENT = True
            verbose("silent output enabled")
        elif argv[n] in ["-T","--threadcount"]:
            global MAXTHREADS
            n += 1
            MAXTHREADS = int(argv[n])
            verbose(f"maximum thread count '{MAXTHREADS}'")
        elif argv[n] in ["-v","--verbose"]:
            global VERBOSE
            VERBOSE = True
            verbose("verbose output enabled")
        elif argv[n] in ["-w","--warning"]:
            global WARNING
            WARNING = False
            verbose("warning output disable")
        elif argv[n].startswith("--fieldsep="):
            global FIELDSEP
            FIELDSEP = argv[n].split("=")[1]
            verbose("field separate is '{FIELDSEP}'")
        elif argv[n].startswith("--recordsep="):
            global RECORDSEP
            RECORDSEP = argv[n].split("=")[1]
            verbose("record separate is '{RECORDSEP}'")
        else:
            DIRECTIVE.append(argv[n])
        n += 1
    if len(DIRECTIVE) == 0:
        syntax()
        exit(E_SYNTAX)
    elif len(DIRECTIVE) == 1:
        args = []
    else:
        args = DIRECTIVE[1:]
    if DIRECTIVE[0] == "help":
        help(args)
        exit(E_OK)
    elif not DIRECTIVE[0] in DIRECTIVES:
        error(f"directive '{DIRECTIVE[0]}' is not valid",E_SYNTAX)
    data = globals()[DIRECTIVE[0]](args)
    if type(data) is pandas.DataFrame:
        if FORMAT == "CSV" or FORMAT.startswith("CSV:"):
            write_csv(data)
        elif FORMAT == "RAW" or FORMAT.startswith("RAW:"):
            write_raw(data)
        elif FORMAT == "JSON" or FORMAT.startswith("JSON:"):
            write_json(data)
        elif FORMAT == "GLM" or FORMAT.startswith("GLM:"):
            write_glm(data)
        elif FORMAT == "POS":
            write_pos(data)
        else:
            error(f"format '{FORMAT}' is invalid",E_SYNTAX)

if __name__ == "__main__":
    main(len(sys.argv),sys.argv)
