[[/Develop/Validation]] - Validation Guide

GridLAB-D validation subsystem is designed to detect code and modeling errors, and identify unexpected changes in the outputs from simulations.  Validation is run as part of the build automation. In addition the `--validate` option can be used to run validation tests locally.

The validation system scan the source tree for all folders named `autotest`.  In any autotest folders, all the files name `test_*.glm` are processed as follows:

1. A subfolder is created using the GLM file's base name

2. The GLM files is copied to the subfolder.

3. The GLM file is run with the command line `gridlabd BASENAME.glm --redirect=all OPTIONS` where OPTIONS are all the options given after the `--validate` command line option.

4. The exit status of the simulation is interpreted as follows

  - If the BASENAME includes the string `_err`, then the exit code must be positive and less than 128 for the validation test to pass, i.e., an error occurred.

  - If the BASENAME includes the string `_exc`, then the exit code must be strictly less than zero or greater than 128, i.e., an exception occurred.

  - If the BASENAME includes the string `_opt`, then the exit status is ignored.

  - Otherwise, the exit status must be exactly zero, i.e., the simulation completed normally.

A report of the validation test results is written to the file `validation.txt`.  Progress followed by a summary is output to the screen.

## Frequently used options

There are a few very useful command line that be used *before* the `--validate` option.

- `-T COUNT` or `--threadcount COUNT`: This options enable parallel processing of the validation tests.  The count `0` indicates the validation system should try to use the maximum number of available processors.  A positive number may be given to indicate the exact number of processors to use. This can reduce the time needed to perform validate tests.

- `-W FOLDER`: This options is used to limit in which folder the search for `autotest` folders is performed. This is useful to perform quick tests of only particular parts of the entire `gridlabd` system.

- `-D keep_progress=TRUE`: This option prevent overwrite of previous progress output. This is useful when performing automated builds where the progress output needs to be preserved.

# Validation Conventions

Validation test emerge from two primary activities.  The first is code development, during test cases are identified to ensure that the code works as intended.  The second is from issues identified by users, when test cases are used to exemplify a problem or scenario that might not work as expected. 

A number of conventions have evolved in validation testing as a result of these activites.

## Use simple models

Some tests need to use very complex models, but most tests should be as simple as possible.

## Use fast tests

Minimize the testing time. There is no need to run a 1 year simulation when a 1 day simulation will suffice.

## Test for edge cases and invalid inputs

Don't just test for what works. Also test for what unusual and what doesn't work.  Make sure you test for strange cases that give unintuitive results, and for bad inputs such out of range values, incompatible units, or utter nonsense. Be creative.

## Check the results

Autotests should use `assert` object or `#on_exit` macros to verify that the results are correct.  Asserts can be used to check values during a simulation, and `#on_exit` can be used to check after completion.

The most common `assert` is simple to very that a value does not go outside a reasonable range, e.g.,

~~~
object house
{
    object assert
    {
        target "air_temperature";
        relation "inside";
        lower 60;
        upper 90;
    };
}
~~~

which causes the simulation to fail if the target value goes outside the specified range.

The most common use of the `#on_exit` macro is to compare an output file to a reference output file, e.g.,

~~~
#define OUTPUT=${modelname/.glm/.csv}
module tape
{
    csv_header_type NAME;
}
module residential;
object house
{
    object recorder
    {
        file ${OUTPUT};
    };
}
#ifexist "../${OUTPUT}"
#on_exit 0 diff -w ../${OUTPUT} ${OUTPUT}
#endif
~~~

which causes the simulation to fail is the recorder output differs from the one provided in the parent autotest folder.  This reference file is automatically generated by the simulation when it is run from the parent folder, so the developer must remember to create and update it whenever the code or model changes in a significant way.

# Automation artifacts

GridLAB-D's automated build system runs validation every time the code is updated.  If the validation test fails, an artifact file is created that can be downloaded.  The artifact file will contain the autotest folder contents created by the validation tests that failed.  These can be examined to determine the cause of the validation test failure.
